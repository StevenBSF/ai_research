多视图聚类旨在有效提取和利用来自多个视角的语义信息，以实现更优的聚类效果。现有方法主要基于深度学习，尽管具有一定优势，但也存在显著局限性。在real-world multi-view scenarios中，深度方法会遇到noisy view dependency和dominant view dependency的问题。首先，这些方法在cross-view层面由于缺少语义过滤，导致模型难以区分semantic factors和noise factors。其次，由于视角之间存在语义差异，这些模型可能会过分依赖某些主导视角，进而难以有效整合各视角的互补信息。为了解决这些局限性，我们提出了一种名为基于因果的多视图聚类（CausalMVC）的新模型。具体来说，为了缓解noisy view dependency的影响，我们引入了因果特征学习，构建dual differential content–style network。该方法侧重于识别因果关系，而非仅仅依赖于深度学习方法中常见的统计相关性。该方法能够过滤掉noisy factors，使模型集中关注真正反映跨视角潜在模式的特征。此外，为了降低dominant view dependency，我们从视内和跨视角两个角度构建了因果内容-风格特征。Moreover, we introduce  content-centered style receptive field module用于对比学习的方法中。与现有仅关注跨视角内容的方法不同，我们的方法既捕捉并平衡了各视角之间共享的内容信息，也保留了每个视角独有的内容和风格，避免了模型过分依赖主导视角。九个数据集上的实验结果证明了所提出模型的有效性。



然而，许多现有的深度多视图聚类方法缺乏有效的语义过滤，无法区分聚类信号与无关的变化。实际上，每个视图的数据通常都混杂着噪声因素和视图特有的失真，即仅存在于该视图中的偏差或扰动。之前的工作\cite{DBLP:conf/cvpr/YanZLT0LL23}往往将所有观察到的跨视图相关性视为同等重要，因此可能会把这些噪声因素当作真实的聚类信号加以强化。结果，这些方法识别出的聚类容易受到视图特有噪声的影响而产生偏差或误导，尤其是在某些视图高度失真或噪声严重的情况下。这一局限性使得多视图聚类技术难以稳健地捕捉到不同视图中真正的潜在聚类结构。



此外，多视图聚类常常面临视图依赖的问题，即每个视图都承载着独特的内容、噪声和偏差。因此，模型往往过度依赖于某些主导视图中的聚类线索。这种过度依赖阻碍了所有视图之间互补信息的有效整合，使模型无法充分利用多视图数据中蕴含的互补信息，从而削弱了多视图数据的潜在优势，降低了聚类任务的整体效果。例如，\cite{DealMVC}在视图差异较小的数据集上取得了良好效果，但在视图差异显著的数据集（如Caltech7，参见表\ref{table:View dependency}中的结果）上却难以获得令人满意的结果。问题的根源在于，深度学习方法在进行跨视图融合时，未能全面考虑各视图间的共同语义信息。将每个视图的特征投影到统一的特征空间，往往会使所有视图被对齐到某些主导视图上，从而导致其他视图的补充信息丢失。在实际应用中，这种做法使模型过分依赖于数据的某些特定视图，忽略了多视图数据的原始特性，最终偏离了多视图学习的本质。



然而，当前深度多视图聚类方法在进行跨视图信息融合时仍存在显著不足，尤其是在视图依赖问题上。本文从总体上分析该问题，并将其主要分为两大方面：噪声视图依赖（noisy view dependency）和主导视图依赖（dominant view dependency）。许多现有深度多视图聚类方法缺乏有效的语义过滤机制，难以将真实的聚类信号与各视图中无关的变化进行区分。实际上，每个视图的数据往往与视图特有的噪声因素和失真（即仅存在于该视图中的偏差或扰动）交织在一起。例如，部分工作\cite{DBLP:conf/cvpr/YanZLT0LL23}倾向于将所有观察到的跨视图相关性视为同等重要，从而可能错误地将噪声因素当作真实的聚类线索加以强化。结果，在某些视图噪声严重或失真较高的情况下，聚类结果容易受到误导或产生偏差，进而影响整个多视图聚类系统捕捉真实聚类结构的能力。

​	2.	**主导视图依赖**

除了噪声干扰外，多视图数据中各视图往往包含各自独特且互补的信息。在实际应用中，由于某些视图在特征表达上更为显著或包含更多的聚类线索，模型在进行跨视图融合时往往会过度依赖这些主导视图的信息。在将各视图特征投影到统一特征空间时，容易出现alignment shift，使得各视图被迫与主导视图对齐。这导致模型忽略了其他视图中的补充信息，并且使得特征中心产生偏移。例如，\cite{DealMVC}在视图差异较小的数据集上表现较好，但在视图差异显著（如Caltech7，参见表\ref{table:View dependency}）的数据集上，其聚类效果明显下降。这说明主导视图依赖削弱了多视图方法对各视图互补信息的充分利用，限制了聚类性能的进一步提升。



MVCAN提出了Noisy View Drawback的问题假设，但是局限于部分视图存在严重噪声干扰。对于当所有视图均受到噪声严重干扰时的适应性存在鲁棒的平衡问题。



To overcome these issues, we propose a novel causal-based multi-view clustering framework (CausalMVC) that effectively Mitigate the NVD and DVD problem mentioned above. The key idea of CausalMVC is to leverage causal representations learning and disentangle adaptive content-style representations to isolate the factors that truly drive clustering, while filtering out noise factors within each view. In our approach, we assume that each data instance is generated by two distinct latent factors: a content representation that captures the essential, cluster-defining features common to all views, and a style representation that reflects the view-specific variations. 我们引入differential content-style network，解耦content和style representation并且去除noise factors。我们从intra-view和cross-view 两个perspective出发，对齐content representation并且丰富化各个视角的style representation。最后我们提出了以content为中心的style reception field的对比学习模块，相较于已有对比方法的工作，能使得正样本对能有更强的语义关联。





	1. 提出CausalMVC模型，将因果机制引入多视图聚类任务。我们设计了一种双重差分网络，用以解耦数据中的共享内容信息和视图特有的风格信息。由于网络结构能有效剔除无关的noise factors，因此模型的噪声视图依赖问题的得到显著降低。

2. 我们从intra-view和cross-view角度，将content representation进行对齐，并且丰富化各个视角的style representation。解决因噪声干扰导致聚类偏差及主导视图依赖问题。

3. 我们构建了一个利用内容中心风格感受野机制的对比学习模块，增强正样本对之间的语义关联，从而在跨视图融合时平衡共享与个性化信息，进一步提升聚类效果和鲁棒性。









#  Cross-view Content Consistency

由于同样的样本的representation在不同视角下可能存在语义上的共享信息和视图差异，即对应Eq.(2)中的content和style的因果建模。针对Multi-view clustering的任务而言，在保证所有视角对样本的内容达成一致的同时，允许每个视角保留各自的风格变化。已有的工作中(GCFAgg)对于跨视角信息使用了transformer架构式的global cross-view 融合，导致representation缺乏有效语义层面的过滤，无法保证融合的特征均对于聚类任务有意义。因此，有效一致化跨视角content representation的同时均衡且多样化 content 信息，并且保证不同视图下的风格的多样性成为难点。因此，我们引入了$\mathcal{L}_{\text{CrossView}}$损失，写为：

\begin{equation}
\label{eq:Crossloss}
\begin{split}
\mathcal{L}_{\text{CrossView}}  :=&  \sum\limits_{1 \leq k < k' \leq V } \mathbb{E}_{(\mathbf{Z}_c^k, \mathbf{Z}_c^{k'}) \sim p_{\mathbf{Z}_c^k, \mathbf{Z}_c^{k'}}} \left[ \left\| \mathbf{Z}_c^k - \mathbf{Z}_c^{k'} \right\|_2^2 \right]\\
 &- \sum^{V}_{v=1} \mathtt{Entropy}(\mathbf{Z}_c^{v}) - \sum^{V}_{v=1} \mathtt{Entropy}(\mathbf{Z}_s^{v}).
\end{split}
\end{equation}

其中第一项促使相同样本在不同视角下的内容特征尽可能相似，使得每个样本都能拥有一致的跨视角内容表示。该项对于聚类的鲁棒性至关重要：它确保了聚类（内容）是基于数据的本质特征，而非某个特定视角的特性。本质上，该项最大化了视角间的内容一致性，确保模型学到的内容表示仅包含跨视角共享的因子，而非特定视角的伪特征。从直觉上来看，确保如果两个数据点在一个视角中被归为同一簇，它们在其他视角中也会被分到相同的簇。$- \sum^{V}_{v=1} \mathtt{Entropy}(\mathbf{Z}_c^{v})$通过最大化每个视角的内容熵，该损失项鼓励均衡且多样化的聚类分配,防止该视角的内容表示不会塌缩至单一类别或无意义的结果。本质上，其使得内容表示具有较高的信息量，并确保簇是分离清晰、且具有实际意义的。$- \sum^{V}_{v=1} \mathtt{Entropy}(\mathbf{Z}_s^{v})$保证了内容表示 $\mathbf{Z}_c$ 只编码跨视角稳定的特征，而样本间的细微差异由风格表示 $\mathbf{Z}_s$ 负责捕捉。通过风格熵项，模型会自发地学习到内容信息是共享的，而风格信息是个性化的。







**通过最小化 SparseCov 实现独立性**



最小化 $\mathcal{L}_{\text{SparseCov}}$ 使得内容和风格特征趋于统计上的独立性。从整体上看，如果所有交叉协方差 $\text{Cov}(Z_c^i, Z_s^j)$ 逐渐趋于 $0$（稀疏性保证）且不存在低维度的残留相关性（秩最小化保证），那么 $Z_c$ 和 $Z_s$ 就会逐渐变得不相关，最终趋于独立。这种解耦正是因果解耦的目标——即内容与风格之间不应共享信息。



该损失的两个组成部分分别起到：

​	•	**第一项**：鼓励交叉协方差矩阵 $\Sigma_{cs}$ 变得稀疏，从而消除每个内容维度与风格维度之间的个体相关性；

​	•	**第二项**：通过降低 $\Sigma_{cs}$ 的秩，确保即便存在残余相关性，也被压缩到极小的维度，使内容与风格特征趋于**统计正交的独立子空间**。



**独立性的两大关键机制**

​	1.	**解耦内容和风格特征**

​	•	**稀疏性项**去除了每个内容特征 $Z_c^i$ 与每个风格特征 $Z_s^j$ 之间的线性相关性，这意味着**无法通过风格特征预测内容特征，反之亦然**。

​	•	由于非零协方差会被惩罚，模型无法在风格变量中隐式地编码内容信息，或者在内容变量中隐式地编码风格信息。

​	•	这迫使模型**将共享因素仅编码到 $Z_c$，将视角特定因素仅编码到 $Z_s$**，从而实现内容-风格的分离。

​	2.	**通过低秩化增强强独立性**

​	•	若仅使用稀疏性惩罚，有可能会出现这样一种病态情况：每个内容特征与风格特征的相关性虽然较小，但仍然存在许多微弱的残留相关性。

​	•	**秩最小化惩罚则能防止这种现象**，因为它倾向于完全消除相关性，而不是仅仅削弱个别相关性。

​	•	在极端情况下，最优解应当是 $\Sigma_{cs} = 0$，即**完全独立**。