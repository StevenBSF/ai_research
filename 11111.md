下面这一路「空白 prediction」的原因，99 %的概率并不在 LoRA 还没学到东西，而是 *推理环节* 本身给了模型一条“马上结束”的指令。常见触发点有两处：

| **触发点**                                            | **现象**                                                     | **解决⽅案**                                                 |
| ----------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **pad_token_id 设成了 eos_token_id**                  | generate() 把 *填充* 和 *终止* 视为同一个 token，一进解码循环就遇到 EOS ⇒ 直接停在首 token，结果 generated_only_ids 长度为 0 ⇒ 解码后就是空串 | 调用 generate() 时改成pad_token_id=tokenizer.pad_token_id    |
| **直接对 language_model.generate() 供 inputs_embeds** | Hugging Face 内部会用 *哑元* input_ids（0, 1, 2, …）替代真实文本 token；第一步 OK，但第二步后开始用这些「假 token」继续预测，极易在首步就给出 EOS | · 更安全的作法是把 **文本 token 的 input_ids** 一并传进去——遇到 32 个音频/图像嵌入的槽位就用 tokenizer.unk_token_id 或 tokenizer.pad_token_id 占位即可；· 或者用 **model_engine.module.generate()**（需让 MultimodalContinualModel 继承 PreTrainedModel 并注册好 config），让框架自动回调你在类里实现的 prepare_inputs_for_generation，就不用 dummy-ids 了 |



------





## **建议的最小改动**



```
# train.py ─ 验证阶段
generated_ids = model_engine.module.language_model.generate(
-   **generation_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id
+   **generation_inputs,
+   max_new_tokens=20,
+   pad_token_id=tokenizer.pad_token_id,   # ← 关键修正
)
```

如果仍然只输出空串，再加一个 sanity-check：

```
with torch.no_grad():
    out = model_engine.module.language_model.generate(
        **generation_inputs,
        max_new_tokens=20,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,   # 显式写明也行
        do_sample=False,   # 先用贪婪搜索排除采样干扰
    )
    print(out[0, generation_inputs["inputs_embeds"].shape[1]:])
```

若首个 token 就是 eos_token_id (通常是 2)，说明问题 ①；

若不是 eos 但仍解码为空，再检查 **dummy input_ids** 逻辑，改为：

```
# ① 给 MultimodalContinualModel 增加一个 create_input_ids() 工具：
def _build_input_ids(self, input_ids, multimodal_len, start_idx, end_idx):
    # 把文本 token 保留，其间 32 个 multimodal 位置用 pad 占位
    left  = input_ids[:start_idx]
    middle = input_ids.new_full((multimodal_len,), self.tokenizer.pad_token_id)
    right = input_ids[end_idx+1:]
    return torch.cat([left, middle, right], dim=0)

# ② prepare_inputs_for_generation() 里返回：
return {
    "inputs_embeds": inputs_embeds,
    "attention_mask": final_attention_mask,
    "input_ids": final_input_ids,   # ← 真实 ids / pad ids 混合后序列
}
```

然后依旧在 **language_model.generate()** 上跑即可——这样就没有 dummy-ids 问题。



------





## **其他可以顺手排查的小点**





1. **tokenizer.add_special_tokens()**

   你新增了 [PAD]，但没有 tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]') 的话，HF 默认可能仍是 None。

2. **max_new_tokens 太低**

   有些开源 LLM 第一步常先输出 BOS，真正内容要到第二个 token 才开始；不妨先设成 max_new_tokens=40 试试看。

3. **验证集 prompt 是否以 <AUDIO_END> 结尾**

   如果文本里最后一个字符就是句号而后没再留空格 / </s>，模型有时会直接结束；可以在拼接时在尾部加一个空格或 \n。

4. **LoRA & dtype**

   LoRA 注入后如果强制 dtype=float16、而 inputs_embeds 还是 bfloat16／float32，有可能导致前向 NaN ⇒ logits 爆炸 ⇒ argmax = eos。确保 prepare_multimodal_inputs() 最后一句 inputs_embeds = inputs_embeds.to(self.language_model.dtype)。





------



按照「先修 pad_token_id → 再看是否仍然即刻输出 EOS → 如仍有问题则加上真实 input_ids」的顺序调试，基本能立刻看到生成文字了。祝顺利！