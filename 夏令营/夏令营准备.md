# 概率论基础

- KL散度
  - https://blog.csdn.net/Rocky6688/article/details/103470437

- https://eggplantisme.github.io/2021/10/04/从熵到费诺不等式-笔记

- 互信息
  - https://blog.csdn.net/luoxuexiong/article/details/113059152

- 协方差
  - https://zhuanlan.zhihu.com/p/57835337

- 矩的概念
  - ![image-20250526213558930](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250526213558930.png)

- ![image-20250528221358423](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250528221358423.png)

  - ![image-20250528222050475](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250528222050475.png)

  - ![image-20250528222110288](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250528222110288.png)

  - ![image-20250528222122519](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250528222122519.png)

  - ![image-20250528222132698](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250528222132698.png)

  - 在模式识别中，我们经常要对各种随机误差、噪声或高维向量的长度做“尾部”（tail）概率估计，来保证我们的分类器或检测器在最坏情况下仍然有足够的性能。下面举几个常见的例子，说明为什么 (a)、(b) 这类正态分布尾部界（exponential tail bound）对模式识别是基础而重要的。

    ### **1. 检测器的虚警率／漏警率控制**

    假设我们要在噪声模型下做二元检测，零假设下观测值 X\sim\mathcal N(0,1)，如果我们用阈值 \varepsilon 做检验（“如果 X\ge\varepsilon 就判定为信号存在”），那么

    P(\text{虚警}) = P_{H_0}(X\ge\varepsilon)\le \tfrac12e^{-\varepsilon^2/2}\,.

    这条指数衰减的上界告诉我们：只要把阈值再抬高一点，虚警率就会以“指数速度”下降——非常有助于设计在严格虚警要求下的检测器。

    ### **2. 分类器的错误率分析**

    在高斯噪声背景下的线性分类器（比如 Fisher 判别或朴素贝叶斯），两个类别的投影分布通常都是均值不同的正态分布。它们的重叠区间对应的“误分类概率”正是两侧尾积分之和，也可用类似的尾部界来快速给出上界，从而评估在不同信噪比（signal‐to‐noise ratio）下的分类错误率。

    ### **3. 支持向量机（SVM）中的余量（margin）集中性**

    现代机器学习的“结构风险最小化”思想里，分类器的泛化能力跟样本点到决策面的余量（margin）密切相关。当我们假设数据加上少量高斯扰动，或者特征本身服从某种近似高斯分布时，尾部界可以用来证明“几乎所有点都不太可能离决策面太近”，从而保证泛化误差上界。

    ### **4. 高维数据的“维度诅咒”与“集中现象”**

    在维度很高的情形下，如果各维都是独立标准正态，那么向量的模长

    \|X\|=\sqrt{X_1^2+\cdots+X_d^2}

    会高度集中在 \sqrt d 附近。利用上面 (b) 的尾部界，可以定量地说明“偏离 \sqrt d 超过某个 \varepsilon”的概率是指数级下降的，这对理解最近邻、距离度量和聚类算法在高维下的失效机理非常关键。

    **小结**：

    正是因为这些指数级的“尾部衰减”性质，我们才能在设计阈值、分析错误率和推导泛化界时，不至于每次都要做难以解析的积分，而是直接套用形式统一、易于理解和优化的上界。因此，(a)、(b) 题所给的正态分布尾部不等式，恰恰是模式识别和统计学习理论中无处不在的“隐形利器”。



# LLM

- Transformer
  - guideline
    - https://jalammar.github.io/illustrated-transformer/
  
  - 聊一聊Transformer中的FFN
    - https://zhuanlan.zhihu.com/p/685943779
  
- ViT
  - https://blog.csdn.net/qq_37541097/article/details/118242600
  - 

# 机器学习

- 西瓜书注解

  https://zhuanlan.zhihu.com/p/134089340



https://yzhu.io/s/research/getting_started/01.ai/



- 动手学强化学习
  - https://hrl.boyuai.com/chapter/intro
- 动手学机器学习
  - https://hml.boyuai.com/books





清华ai:

您认识推荐人的方式、认识的时间和了解的程度；并请对推荐人思想品德、道德修养方面加以介绍：

对推荐人学术水平、科研能力、研究成果、知识结构、外语水平等的介绍：

推荐人的特长和弱点，从课程学习和从事科研工作（如果有的话）的情况看，该推荐人是否具有创新的潜力：

中科院:

对被推荐人思想品德、专业学习和科研能力、外语水平、研究成果等的介绍：

北京大学:

对被推荐人思想品德、专业学习和科研能力、外语水平、研究成果等的介绍：

对申请人外语水平、知识结构、科研能力、工作成果、以及申请人所应具备的博士生素质和培养潜力简要评价:

人大:

请概括考生的主要学术特长及不足（包括考生专业研究能力、科研成果和外语水平等方面的评语）：
