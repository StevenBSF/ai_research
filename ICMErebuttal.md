- 研究动机、问题陈述与整体清晰度

  我们研究动机主要聚焦于两个方面：

  ​	1.	fine-grained semantic control:

  现有的多视图聚类方法只在粗粒度或全局层面对特征进行融合，忽视了对准确聚类至关重要的细微差别和视图特定细节。当数据复杂或高维时，这种局限性更可能导致子最优表现。分层 Prompt注入更精细的、针对不同视图的可学习参数，从多个层面提升语义提取能力。

  ​	2.	instability in alignment

  仅使用单向或纯特征级别的对齐，会要求各视图严格匹配，忽略了它们本身的特有差异。结果使得模型收敛到局部最优，出现某些视图支配或淹没其它视图的情况。bidirectional alignment通过正向和反向反馈的方式，灵活地使共享的聚类结构（全局分布）和各视图的局部结构在训练过程中保持一致。

  

  基于此，我们在 **HiPMVC** 中引入了分层 Prompt（主级与嵌入级）和双向对齐机制，以解决上述问题：

  ​	•	**Prompt 学习模块**：注入更精细的、针对不同视图的可学习参数，从多个层面提升语义提取能力。

  ​	•	**双向对齐（bidirectional alignment）**：通过正向和反向反馈的方式，灵活地使共享的聚类结构（全局分布）和各视图的局部结构在训练过程中保持一致。

  - 提示学习相关的介绍和动机偏少

- Comparisons with Prompt-Based Methods(R1)

  目前还没有现成的研究将提示学习直接应用于多视图聚类任务，我们的工作是首个在多视图聚类任务中引入 Prompt 概念的尝试。许多现有的 Prompt 学习方法大多聚焦于有监督的大规模模型（如视觉-语言模型）或下游分类任务，尚无法直接适用于我们所研究的无监督多视图聚类场景。在修订稿中我们会更强调这一点。

- 双向 Prompt 引导的对齐的必要性
  - 补充illustration的图？
  
- 关于 $L_{\text{forward}}$ 的替代形式R9
  - 包含 -H(Y_P) 能使 Prompt 级表示 Y_P 更均匀地利用所有聚类，避免塌缩到单一聚类上。如table1所示，实验中，我们发现增加-H(Y_U)后，聚类纯度会略有下降，这是因为 Prompt 级的聚类概率分布就可能变得过度集中，无法充分利用其它潜在聚类空间，导致一些原本可以区分的样本被错误地划分进同一个类。
  - 补充消融实验，替代形式效果更差
  
- *L_forward和L_backward的设计理由*
  
  L_forward（公式13）通过Y_all强调主导簇，并抑制熵以避免平庸解。其中H(Y_P)确保了全局提示的多样性。L_backward（公式16）通过中间映射平衡了全局和局部分布。
  
- 消融实验补充
  
  如table1和table2所示，关于L_forward和各个层级单独的prompt效果已补充。
  
- 初始化和超参数

- We initialize learnable prompts from a Gaussian distribution with \sigma=0.02. Eq.5中$\lambda$ 置为0.5。\beta, \gamma 通过网格搜索{0.01, 0.1, 1, 10} ,经过调优发现对于参数并不敏感，因此选择$\beta=1,\gamma=1$。

- 记号修正
  - $P_I$和$P_L$
  - 六个数据集写成9个 





​	•	在相关工作中提到，现有的提示学习方法通常将可学习提示作为**预训练冻结编码器**（如CLIP中的ViT-B/16）的输入，而本文未提及引入此类预训练编码器。由于这一设计与典型的提示学习方法（如CoOp、PromptSRC）不同，建议进一步讨论HiPMVC中提示学习模块的工作机制。例如，当编码器未冻结时，所提方法如何确保可学习提示在优化过程中的鲁棒性？

说明一下我们的提示的区别，有别于大模型

引用相关文献

工作机制，两三句话解释，引用

例如，当编码器未冻结时，所提方法如何确保可学习提示在优化过程中的鲁棒性？

网络规模较小，不是大模型encoder，不需要做冻结编码器，直接做提示学习优化就可以获得比较理想的效果。





![image-20250217212945481](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250217212945481.png)



**问题**

​	1.	您对该论文评审的信心度如何？

**信心较低**

​	2.	该论文对ICME的重要性/相关性如何？

**兴趣有限**

​	3.	对重要性/相关性的理由

论文未明确说明所使用的数据类型，也未详细说明所提方法如何应用于多模态数据，因此，该方法的整体重要性尚不清晰。

​	4.	新颖性/原创性如何？

**中等原创性**

​	5.	对新颖性/原创性的理由

本文提出的分层提示多视角聚类方法具有一定的创新性，且其底层机制通过大量实验得到了充分验证。

​	6.	技术正确性如何？

**可能正确**

​	7.	对技术正确性的理由

论文清晰地展示了其机制，各模块均经过了充分的理论分析和实验验证，使得方法具有较高的说服力。然而，分层提示在特征表示方面的重要性似乎未得到充分提炼。

​	8.	实验验证和可复现性如何？

**有限但令人信服**

​	9.	对实验验证和可复现性的理由

论文中所展示的实验较为全面，且相较于现有方法具有显著提升，但仍存在一些问题需要注意。消融实验的结果仅展示了各个消融版本，缺乏与完整未消融模型的直接对比，影响了论文的可读性。此外，鉴于所提方法相较以往技术有显著提升，建议作者公开源代码，以促进该领域的进一步研究。

​	10.	论文的表达清晰度如何？

**足够清晰**

​	11.	对表达清晰度的理由

论文整体写作流畅，逻辑清晰，但需要在全文中更明确地突出其对多模态领域的贡献。

​	12.	参考文献的充分性如何？

**参考文献充足**

​	13.	对参考文献的理由

相关工作综述清晰，逻辑性较强。

​	14.	论文的整体评价

**弱接受**

​	15.	对整体评价的理由（必填）

该论文逻辑连贯，清晰地展示了其创新方法背后的合理性，但仍有几个方面需要改进。首先，关于提示学习的讨论较为浅显，论文未充分说明提示是如何增强聚类方法的。其次，实验结果的展示需要进一步改进。









**问题**

​	1.	您对该论文评审的信心度如何？

**自信**

​	2.	该论文对ICME的重要性/相关性如何？

**具有广泛兴趣**

​	3.	对重要性/相关性的理由

多视角聚类研究在多媒体应用中具有重要意义。本文研究内容契合ICME的主题。

​	4.	新颖性/原创性如何？

**非常原创**

​	5.	对新颖性/原创性的理由

所提出的HiPMVC将提示学习引入多视角聚类，具有新颖性。其分层框架设计和双向对齐机制实现了多尺度的聚类，同时，HiPMVC支持多种编码器（如MLP和GCN），显示出良好的兼容性。

​	6.	技术正确性如何？

**可能正确**

​	7.	对技术正确性的理由

HiPMVC框架的分层设计具有重要意义，逻辑结构合理。然而，论文在以下方面仍有改进空间：



​	•	在多视角聚类中引入提示学习的动机和理论基础讨论不足。如能解释为何该方法比其他特征融合方法（如多尺度注意力）更适用，将提升论文的说服力。

​	•	在相关工作中提到，现有的提示学习方法通常将可学习提示作为**预训练冻结编码器**（如CLIP中的ViT-B/16）的输入，而本文未提及引入此类预训练编码器。由于这一设计与典型的提示学习方法（如CoOp、PromptSRC）不同，建议进一步讨论HiPMVC中提示学习模块的工作机制。例如，当编码器未冻结时，所提方法如何确保可学习提示在优化过程中的鲁棒性？



​	8.	实验验证和可复现性如何？

**验证充分/理论性论文**

​	9.	对实验验证和可复现性的理由

实验全面且完善，涵盖了大规模性能对比、消融研究和可视化分析。以下为改进建议：



​	•	建议提供更多关键实验参数的细节。例如，每个层级引入的可学习提示采用何种数据分布进行初始化？输入数据嵌入的维度和MLP层的结构是什么？

​	•	在第IV-A节存在一个小的拼写错误：论文声称实验基于9个多视角数据集进行，但实际数量应为6个。



​	10.	论文的表达清晰度如何？

**较难阅读**

​	11.	对表达清晰度的理由

尽管论文的整体结构清晰，但第III节中的部分符号表示较为混乱，影响了对论文的理解。例如，在第III-A节中，符号\{P_I\}^v和\{P_L\}^v似乎混淆了。与第III-B节的定义相比，这里的\{P_L\}^v是否应改为\{P_I\}^v？同样，在(5)式和(6)式之间的段落中，P_u是否应修正为H_u？

​	12.	参考文献的充分性如何？

**参考文献极佳**

​	13.	对参考文献的理由

论文正确引入了多视角聚类领域的相关且最新的文献。

​	14.	论文的整体评价

**边缘通过**

​	15.	对整体评价的理由（必填）

本文提出了HiPMVC，一个结合分层提示学习和双向提示引导对齐的新型多视角聚类框架。尽管实验结果较强，但提示学习的理论基础和工作机制尚未详细说明。此外，部分符号错误影响了论文的清晰性。若能解决这些问题，将为多视角聚类和提示学习研究领域提供有价值的新见解。





![image-20250217213136048](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250217213136048.png)

![image-20250217213147184](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250217213147184.png)

![image-20250217213206955](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250217213206955.png)







R9：

![image-20250217212849087](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250217212849087.png)