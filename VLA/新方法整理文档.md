# **基于时序提示的结构感知潜在动作表示学习与无监督动作单元划分**



## 相关工作

视觉-语言-动作（VLA）模型旨在学习从视觉观测和语言指令到动作序列的映射。在无标注视频数据上进行预训练可以大规模利用互联网视频丰富的动作信息，但现有方法各有局限。LAPA（Latent Action Pretraining from Videos）提出了无监督预训练VLA模型的方法，通过VQ-VAE目标从连续视频帧中学习离散的潜在动作表示  ；但该方法只对固定帧对进行全局量化，未显式建模更长序列中的动作边界和结构。STOP（Spatial-Temporal Prompting）则引入了帧内空间提示和帧间时间提示机制，通过动态在高变化帧间插入提示，引导模型关注时序动态显著的关键帧  ；但STOP主要用于视频-文本匹配等任务，其提示策略设计并未直接用于动作预测或分割，缺乏对无监督动作单元划分的支持。此外，UVA（Unified Video Action Model）提出了视频和动作的联合潜在表示与解耦解码策略，在机器人多任务学习中将视频生成与动作预测集成 ；GEVRM通过基于文本的视频生成目标和扰动对比学习增强闭环VLA模型的鲁棒性 。然而，这些模型通常依赖有标签的机器人轨迹或规划模块，针对无标签视频场景中的动作分割问题并不适用。综上，现有工作缺乏无监督条件下结构化动作边界建模和原子动作单元发现的机制。



## 方法动机

复杂动作可视为由多个原子动作片段按序组成。一个有效的潜在动作表示应能捕获局部帧间的转移动力学并具有**可组合性**，以便复用不同局部模式生成更长动作序列。然而，现有方法如LAPA仅基于帧对离散化而忽略更长时序上下文  。另一方面，在无标注数据中，**动作单元的起始和结束边界**对动作理解至关重要，但缺乏直接标注使边界识别极具挑战。基于此，我们提出的目标是：①学习一种能够在未标注视频中对局部帧转移进行结构感知编码的潜在动作表示；②设计一种自监督的边界发现机制，利用模型自身信息识别潜在动作单元的起止帧并引导模型学习这些边界。为此，我们提出两大核心机制：“结构感知潜在动作表示学习”与“提示感知的边界发现”，分别解决上述两个问题。



## **3 方法**

### **3.1 结构感知潜在动作表征学习**

### **Structure-Aware Latent Action Modeling**

#### **3.1.1 帧特征与时序编码**

**输入划分与编码**

把第 t 帧图像 $I_t$ 切成 $N_p$ 个小 Patch，并投影成 $d$ 维向量 → 得到尺寸为 ($N_p \times d$) 的**帧内 patch 特征矩阵** $\mathbf{x}_t$。$f_{patch}$ 是 C-ViViT 的 Patch-Embedding 模块。

$$\mathbf{x}_t \;=\; f_{\text{patch}}(I_t)\in \mathbb{R}^{N_p\times d} \tag{1}$$

**滑动窗口时序编码**

取长度为 $K$ 的滑动窗口$\mathcal{W}_t=\{\mathbf{x}_t,\ldots,\mathbf{x}_{t+K-1}\}$（第 $t$ 帧到第 $t+K-1$ 帧），送入时序 Transformer $f_{\text{temp}}$。它会让这 K 帧之间互相“看”，得到融合时空信息的张量 $\mathbf{Z}_t$.在时间轴上把 **连续 K 帧** 一起编码，捕获它们的动态关联。

$$\mathbf{Z}_t \;=\; f_{\text{temp}}\!\bigl(\mathbf{x}_t,\ldots,\mathbf{x}_{t+K-1}\bigr)\in \mathbb{R}^{K\times N_p\times d}$$

首尾两帧的 Patch 差分 $\Delta\mathbf{x}_t$ *反映整体运动；把它丢给小网络* $g_{\text{act}}$（几层卷积/MLP）得到 运动激活图 $\mathbf{A}^s_t$，长度为 patch 数 $N_p$，表示每个 patch 运动剧烈程度.

$$\Delta\mathbf{x}_t = \mathbf{x}_{t+K-1}-\mathbf{x}_t, \quad \mathbf{A}^s_t = g_{\text{act}}(\Delta\mathbf{x}_t)\in\mathbb{R}^{N_p} $$

#### **3.1.2 提示生成**

1. **帧内空间提示**

将激活图再映射到与 patch 特征同维度的矩阵 $P^s_t$，称为**帧内空间提示**。每一行对应一个 patch 的“提示向量”。

$$P^s_t = g_s\bigl(\mathbf{A}^s_t\bigr)\in\mathbb{R}^{N_p\times d}$$

把提示直接加到原 patch 特征，相当于给“动得大的”区域增加权重,使得让后续注意力更关注这些区域。

$$\tilde{\mathbf{x}}_t = \mathbf{x}_t + P^s_t$$

2. **帧间时间提示**

把差分矩阵整体池化成标量 $\delta_t$ 表明窗口的运动强度，然后用 MLP $g_t$ 生成 **时序提示向量** $p^{\,t}$。

$$\delta_t = \text{GlobalPool}\!\bigl(\Delta\mathbf{x}_t\bigr)\in\mathbb{R}, \quad p^{\,t} = g_t(\delta_t)\in\mathbb{R}^{d} $$

在输入序列最前面插入全局时序提示 Token $p^{\,t}$；随后是加了空间提示的各帧 patch Token。从而使得backbone能够感知这段窗口整体动了多少，可能是转折点。

将 $p^{\,t}$ 作为全局Token拼接：

$$\text{Seq}_t = \bigl[p^{\,t},\tilde{\mathbf{x}}_t,\ldots,\tilde{\mathbf{x}}_{t+K-1}\bigr] $$

#### **3.1.3 局部段量化**

**窗口平均向量**

把窗口 $\mathbf{Z}_t$ 在帧和patch两个维度求平均,表示该 **K 帧动作片段的整体特征**。

$$\bar{\mathbf{z}}_t =\frac{1}{K\!N_p}\sum_{i=0}^{K-1}\sum_{j=1}^{N_p}\mathbf{Z}_{t+i,j} $$

**向量量化**

有一个离散“码本”$\mathcal{C}$。选出离 $\bar{\mathbf{z}}_t$ 最近的向量当作其 **潜在动作 Token**（离散 ID）。这样所有片段都映射到有限个离散动作符号。

$$\mathbf{c}_t = \operatorname*{arg\,min}{\mathbf{e}_k\in\mathcal{C}} \bigl\|\bar{\mathbf{z}}_t-\mathbf{e}_k\bigr\|_2^2$$

#### **3.1.4 VQ-VAE 目标**

1. **重建损失**

解码器把（当前帧+量化向量）还原出预测帧 \hat I_{t+K}。用像素级 L2 误差衡量 → **重建损失**。

$$\mathcal{L}_{\text{rec}}= \bigl\| I_{t+K}-\hat I_{t+K}\bigr\|_2^2$$

2. **量化损失**

两项：①让 encoded 结果接近码本向量（sg 阶段不回传），②让码本向量也靠近 encoded 向量（承诺损失乘 β）。保证离散码本被充分利用且稳定。

$$\mathcal{L}_{\text{VQ}}= \bigl\|\text{sg}(\bar{\mathbf{z}}t)-\mathbf{e}{c_t}\bigr\|_2^2 +\beta\,\bigl\|\bar{\mathbf{z}}t-\text{sg}(\mathbf{e}{c_t})\bigr\|_2^2 $$

### **3.2 提示感知边界发现**

### Prompt-Aware Boundary Discovery

#### **3.2.1 注意力突变检测**

在第 L 层 Transformer、所有 H 个头上，统计**帧 t ↔ 帧 t+1** 所有 patch 之间的平均注意力。值小 → 帧间相似度低 → 可能换动作。

$$\alpha_t= \frac{1}{H N_p^2}\sum_{h=1}^{H}\sum_{i,j} A^{(L,h)}_{t,i;\,t+1,j}$$

若相似度跌破阈值 \tau，打上**边界伪标签**。

$$\hat b_t = \begin{cases} 1,&\alpha_t<\tau\\[4pt] 0,&\text{otherwise} \end{cases}$$

#### **3.2.2 边界提示**

- $p^{\text{temp}}_t$：用边界处两帧特征差 $\Delta\mathbf{z}_t$ 生成**时间提示**,表示动作更换。

- $p^{\text{spa}}_t$：取边界前帧里动得最大的若干 patch（Top-K），生成**空间提示**,表示局部动作特征。

$$p^{\text{temp}}_t = h_\theta\!\bigl(\mathbf{Z}_{t}-\mathbf{Z}_{t+1}\bigr), \quad p^{\text{spa}}_t = h_\phi\!\bigl(\text{TopK}(\mathbf{A}^s_t)\bigr)$$

把两类提示 token 插进 patch token 流 ,以输入下一层 Transformer 自然感知边界。

插入顺序为:

$$\ldots,\tilde{\mathbf{x}}_t, \,p^{\text{spa}}_t,\,p^{\text{temp}}_t, \,\tilde{\mathbf{x}}_{t+1},\ldots $$

#### **3.2.3 边界监督**

用伪标签 $\hat b_t$ 训练一个轻量 MLP 预测边界概率 $p_t$。交叉熵让模型学会区分“平滑过渡”与“关键切换”。$p_t$ 由 Sigmoid 输出。

$$\mathcal{L}_{\text{bdy}}= -\!\sum_t\!\Bigl[ \hat b_t\log p_t+(1-\hat b_t)\log(1-p_t)\Bigr], \quad p_t=\sigma\!\bigl(\mathbf{w}^\top[\bar{\mathbf{z}}t;\bar{\mathbf{z}}{t+1}]\bigr) $$

### **3.3 联合优化**

把所有损失加权：

• 重建 + 量化 —— 让离散 token 真实可用；

• 边界损失 —— 学会分段；

• $\mathcal{L}_{\text{con}}$*（段内一致/段间分离）—— 保证同段特征靠近，不同段远离；*

*•* $\mathcal{L}_{\text{pred}}$ —— 在 VLM 阶段预测离散动作 token 的交叉熵。

$$\mathcal{L}= \mathcal{L}_{\text{rec}} +	\lambda_{vq}\,\mathcal{L}_{\text{VQ}} +	\lambda_{b}\,\mathcal{L}_{\text{bdy}} +	\lambda_{c}\,\mathcal{L}_{\text{con}} +	\lambda_{p}\,\mathcal{L}_{\text{pred}}$$





### **3.4 多阶段提示贯通**

- **阶段-1（SLAM）**：注入 P^s_t, p^{\,t}, p^{\text{temp}}_t, p^{\text{spa}}_t
- **阶段-2（VLM）**：将 p^{\text{temp}}_t 等转换为可训练 soft-prompt，拼接到视觉 / 语言 token 序列，为动作 token 预测提供时序上下文







# 区别和创新点

- LAPA 只对两帧差分作 VQ-VAE；我们改为对长度 K 的滑动段做量化，并引入段内一致/段间分离对比损失,使得潜在动作 token 自带局部结构
  - 让 token 具有可组合性，支持长序列拼接；也能自然对齐动作边界
- STOP 的时间提示用于视频-文本匹配，是静态插入；我们利用 Transformer 多头注意力突变自动产出伪边界标签，并即时生成 $p^{temp} / p^{spa}$ 两类提示 token
  - 无需任何动作标注即可定位边界；提示与编码器闭环，自适应强化边界特征
- 现有 LAPA 没有提示，STOP 只在视觉分支上的提示；我们把同一套提示在第一阶段强化表征，在第二阶段 作为 soft-prompt 进入 LLaMA-2
  - 让时序线索一直保留到动作 token 预测，提升语言条件下的动作生成准确度







# 附

![image-20250526004144345](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250526004144345.png)

Latent Action Pretraining from Videos (ICLR 2025)

**Latent Action Quantization** 仅需原始视频帧（两帧窗口），**不需要任何动作标签**；甚至不必配对语言

   用 VQ-VAE 把两帧之间的视觉差异量化成离散 latent action token

**Latent Pretraining**   视频 + 语言描述（或其他任务指令）；**仍然不需要动作标签**

   训练 VLM 根据当前帧和语言去预测上一步得到的 latent action

**Action Finetuning**         少量机器人动作标注

   仅这一小阶段把 latent action 映射成真实机器人控制量



![image-20250526004952207](/Users/baoshifeng/Library/Application Support/typora-user-images/image-20250526004952207.png)

STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding (ICML 2025)

**STOP**（Spatial-Temporal dynamic Prompting）是一种基于大规模图像-文本预训练模型（如CLIP）的视 视频任务适配方法，它通过插入动态的帧内（空间）和帧间（时间）提示token，使模型聚焦于具有显著运动变化的区域与关键帧，从而提升视频理解效果

STOP模型包括**帧内空间提示**和**帧间时间提示**两部分 









做视频理解的工作调研一下,切分
